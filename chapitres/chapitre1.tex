\documentclass[main.tex]{subfiles}

\chapter{Analyse des solutions existantes}

\section{AlphaGo}

AlphaGo est un programme informatique capable de jouer au jeu de Go\footnote{jeu de stratégie au tour par tour traditionnellement chinois}. En octobre 2015, il devient le premier programme à battre le joueur professionnel français Fen Hui, en mars 2016, il bat l'un des meilleurs joueurs mondiales Lee Sedol et enfin, en mai 2017 il bat le champion du monde, Ke Jie, avant d'être mis à la retraite.

Il a été développé par l'entreprise Britannique DeepMind Technologies. Son algorithme combine des techniques de parcours de graphe et d'apprentissage automatique, à partir de bataille contre d'autres humains, d'autres ordinateurs mais surtout contre lui-même.

En octobre 2017, son algorithme est amélioré dans la version AlphaGo Zero qui atteint un niveau supérieur en jouant uniquement contre lui-même et en décembre 2017 devient capable de battre tout les joueurs et ordinateurs au go, mais aussi aux échecs et au shogi, et ce, toujours par auto-apprentissage.

Les premières versions d'AlphaGo ont été réalisé avec l'utilisation de la méthode de Monte-Carlo, guidé par un réseau de valeur et un réseau d'objectifs, implémentés en utilisant un réseau de neuronne profond. Il a été entraîné pour imiter le joueur humain, en retrouvant la réponse aux coups dans toutes les parties qu'il a enregistrés. Passé un certain niveau, il s'est entraîné contre lui-même utilisant l'apprentissage par renforcement pour s'améliorer.

En revanche, dans une nouvelle étude, par Nature, DeepMind révèle que la version AlphaGo Zero utilise une architecture plus simple, n'utilise plus la méthode de Monte-Carlo, ni de connaissances humaines mais parvient tout de même à atteindre un meilleur niveau que ses versions précédentes.

\subsection{La méthode de Monte-Carlo}

La méthode de recherche arborescente Monte-Carlo désigne une famille de méthode algorithmiques visant à calculer une valeur numérique approchée en utilisant des procédés aléatoires. Elle est fréquemment utilisé dans les jeux tel que Total War : Rome II, ou dans notre cas les échecs et le go.

La méthode Monte-Carlo explore l'arbre des possibles. À la racine de l'arbre se trouve la configuration initiale du jeu. Chaque nœud est une nouvelle configuration et ses enfants sont les configurations suivantes. Monte-Carlo conserve en mémoire un arbre qui correspond aux noeuds déjà explorés de l'arbre des possibles. Une feuille de cet arbre est soit une configuration finale (si l'un des joueurs a gagné ou s'il y a match nul), soit une configuration à partir de laquelle aucune simulation n'a encore été lancée. Dans chaque nœud, on stocke deux nombres : le nombre de simulations gagnantes et le nombre total de simulations. La méthode Monte-Carlo se déroule en quatre étapes :

\begin{itemize}

    \item Sélection :
    
    depuis la racine, on sélectionne successivement des enfants jusqu'à atteindre la fin de l'arbre. Dans cette phase, le choix des enfants est guidé par un compromis entre exploitation (choisir un noeud prouvé comme étant prometteur) et exploration (choisir un noeud moins prometteur en apparence afin de découvrir de nouvelles configurations qui peuvent se révéler être intéressant).
    
    \item Expansion :
    
    si cette configuration n'est pas finale, créer de nouveaux enfants à mesure du possible en utilisant les règles du jeu et choisir l'un des enfants.
    
    \item Simulation :
    
    simuler une partie au hasard depuis cet enfant, jusqu'à atteindre une configuration finale.
    
    \item Rétro-propagation :
    
    utiliser le résultat de la partie au hasard et mettre à jour les informations de manière rétro-active sur la branche en partant du noeud enfant jusqu'à la racine.
    
\end{itemize}

Dans AlphaGo, Monte-Carlo s'aidait d'un réseau de neurones entraîné par des parties d'experts ainsi que du renforcement en se combattant lui-même. Seulement, ce dispositif était limité de par le fait que les données d'experts sont souvent rares et surtout, elles sont onéreuses. Dans la version Zero, il se débarrasse de la méthode de recherche arborescente Monte-Carlo pour un autre algorithme de recherche arborescente qui va s'abstraire de toutes connaissances, comme des parties étudiées, sauf les règles du jeu.

\subsection{Zero}

AlphaZero sera la version finale de AlphaGo. Débarrassé de la complexité de la recherche arborescente de Monte-Carlo et du réseau de neurones qui lui sert à étudier les parties d'experts, AlphaZero devient plus léger et plus généraliste dans sa façon d'apprendre.

La version AlphaGo Zero se base uniquement sur des parties jouées contre elle-même en utilisant un réseau de neurones qui concentre en un seul les deux réseaux de valeurs et d'objectifs qu'utilisait AlphaGo. De plus, elle utilise un nouvel algorithme d'apprentissage où elle apprend de ses propres mouvements, jusqu'à ce qu'elle puisse les anticiper et calculer de ces mouvements et leur impact sur le jeu. Cette version ne se sert plus de parties jouées par des experts, ni de l'aide humaine.

La version d'AlphaZero reprend tout ces principes à quelques différences près :

\begin{itemize}
    \item AlphaZero a des règles codés en dur pour l'algorithme de recherche
    \item Le réseau de neurones est mis à jour de manière constante
    \item AlphaZero n'est pas programmé pour prendre avantage de la symétrie présente dans le Go
    \item AlphaZero prend en compte les matchs nuls
\end{itemize}

https://fr.wikipedia.org/wiki/Recherche_arborescente_Monte-Carlo

https://www.nature.com/articles/nature16961

https://www.nature.com/articles/nature24270

http://aigamedev.com/open/coverage/mcts-rome-ii/

\section{L'intelligence artificielle de manière plus général}



https://www.univ-rennes1.fr/actualites/04042018/elisa-fromont-et-les-defis-de-lintelligence-artificielle