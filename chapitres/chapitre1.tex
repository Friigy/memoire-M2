\documentclass[main.tex]{subfiles}

\chapter{Analyse des solutions existantes}

\section{L'intelligence artificielle de manière plus générale}

L'intelligence artificielle est l'ensemble de théories et de techniques mises en oeuvre en vue de réaliser des machines capables de simuler l'intelligence humaine\cite{IA}. Selon John McCarthy, l'un des créateurs de ce concept, toute activité intellectuelle peut être décrite avec suffisamment de précision pour être simulée par une machine.

Le but de l'intelligence artificielle est donc de penser comme l'humain, ou tout du moins de résoudre les problèmes comme l'humain. Il y a deux approches dans la résolution de problèmes : celle de l'algorithme et de l'heuristique.

\subsection{L'algorithme et l'heuristique}

Un algorithme est une procédure mathématique de résolution. Il s'agit d'une méthode systématique qui donne des résultats fiables. L'algorithme reste cependant limité dans cette lourdeur déterministe. L'heuristique est une méthode stratégique indirecte qui résulte du choix qui parait plus efficace parmi les approches de la solution. Lorsque le problème est complexe, qu'il y a plein d'approches et de résultat différents, la méthode heuristique gagne un temps considérable sur l'algorithme.

\subsection{Le réseau de neurones}

Pour plus se rapprocher de la méthode de résolution de l'humain, Warren McCulloch et Walter Pitts ont proposé de simuler le fonctionnement du système nerveux qui se partagera en trois couches :

\begin{itemize}
    
    \item une couche "spécialisée" dans la réception de stimulus
    
    \item une couche intermédiaire transmettant l'excitation
    
    \item une couche formant la réponse.
    
\end{itemize}

L'idée en revanche n'est pas de traiter l'information et de calculer le résultat plus vite, mais de s'auto organiser, et de traiter plus d'informations et de les calculer en plus grand nombre en même temps.

\subsection{Intelligences artificielles forte et faible}

Il existe deux concepts dans l'intelligence artificielle, les concepts de l'intelligence artificielle forte et de l'intelligence artificielle faible\cite{IAFF}.

L'intelligence artificielle forte a le but d'être intelligente, d'éprouver une vraie conscience de soi d'avoir un raisonnement pas juste injecté, mais qu'elle peut comprendre elle-même. Sa finalité est de lui "donner un esprit".

L'intelligence artificielle faible en revanche existe comme un automate. Elle remplit une tâche, mais de manière plus pragmatique. Elle cherche à trouver une solution et l'appliquer plutôt que de suivre un fil de décision. Au contraire de l'intelligence forte, plutôt que de chercher à être humain, l'intelligence artificielle faible va chercher à l'imiter lors de la résolution d'un problème.

\subsection{Le Machine Learning}

Le machine learning ou "l'apprentissage automatique" est l'un des champs d'études de l'intelligence artificielle qui se fonde sur des approches statistiques pour donner aux ordinateurs la capacité d'apprendre à partir de données\cite{ML}.

La première phase du machine learning consiste à déterminer un modèle à partir de données lors de la phase de conception du système. Cette phase est la phase d'apprentissage ou la machine va apprendre à estimer, reconnaitre ou reproduire. Cette phase est généralement réalisée avant l'utilisation pratique du modèle.

L'apprentissage peut se faire de plusieurs manières :

\begin{itemize}
    
    \item Apprentissage supervisé
    
    Les classes de données sont prédéterminées et les exemples sont connus. Le système apprend à classer selon une classification ou un classement.
    
    \item Apprentissage non supervisé (ou clustering en anglais)
    
    Le système ne dispose que d'exemple non classé ou prédéterminé, le système apprend par lui-même la structure cachée des données.
    
    \item Apprentissage semi-supervisé
    
    Le système apprend par paquets d'exemples sous-jacents à leur espace de description.
    
    \item Apprentissage partiellement supervisé
    
    Le système apprend à la fois avec des données classées et non classées.
    
    \item Apprentissage par renforcement
    
    Le système apprend par un comportement étant donné l'observation. L'action de l'algorithme sur l'environnement produit une valeur de retour qui va le guider dans son apprentissage.
    
    \item Apprentissage par transfert
    
    Le système applique des connaissances et des compétences apprises à partir de tâches antérieures sur de nouvelles tâches ou dans de nouveaux domaines partageant des similitudes.
    
\end{itemize}

La seconde phase consiste en la mise en production. En pratique, le système va continuer d'apprendre durant sa mise en production.

\section{AlphaGo}

AlphaGo est un programme informatique capable de jouer au jeu de Go\footnote{jeu de stratégie au tour par tour traditionnellement chinois}. En octobre 2015, il devient le premier programme à battre le joueur professionnel français Fen Hui, en mars 2016, il bat l'un des meilleurs joueurs mondiaux Lee Sedol et enfin, en mai 2017 il bat le champion du monde, Ke Jie, avant d'être mis à la retraite.

Il a été développé par l'entreprise britannique DeepMind Technologies. Son algorithme combine des techniques de parcours de graphe et d'apprentissage automatique, à partir de bataille contre d'autres humains, d'autres ordinateurs, mais surtout contre lui-même.

En octobre 2017, son algorithme est amélioré dans la version AlphaGo Zero qui atteint un niveau supérieur en jouant uniquement contre lui-même et en décembre 2017 devient capable de battre tous les joueurs et ordinateurs au go, mais aussi aux échecs et au shogi, et ce, toujours par autoapprentissage.

Les premières versions d'AlphaGo\cite{ALPHAGO} ont été réalisées avec l'utilisation de la méthode de Monte-Carlo, guidé par un réseau de valeur et un réseau d'objectifs, implémentés en utilisant un réseau de neurones profond. Il a été entrainé pour imiter le joueur humain, en retrouvant la réponse aux coups dans toutes les parties qu'il a enregistrés. Passé un certain niveau, il s'est entrainé contre lui-même utilisant l'apprentissage par renforcement pour s'améliorer.

En revanche, dans une nouvelle étude, par Nature, DeepMind révèle que la version AlphaGo Zero utilise une architecture plus simple, n'utilise plus la méthode de Monte-Carlo, ni de connaissances humaines, mais parvient tout de même à atteindre un meilleur niveau que ses versions précédentes.

\subsection{La méthode de Monte-Carlo}

La méthode de recherche arborescente Monte-Carlo désigne une famille de méthode algorithmique visant à calculer une valeur numérique approchée en utilisant des procédés aléatoires. Elle est fréquemment utilisée dans les jeux tels que Total War : Rome II\cite{TWR}, ou dans notre cas les échecs et le go.

La méthode Monte-Carlo explore l'arbre des possibles. À la racine de l'arbre se trouve la configuration initiale du jeu. Chaque nœud est une nouvelle configuration et ses enfants sont les configurations suivantes. Monte-Carlo conserve en mémoire un arbre qui correspond aux noeuds déjà explorés de l'arbre des possibles. Une feuille de cet arbre est soit une configuration finale (si l'un des joueurs a gagné ou s'il y a match nul), soit une configuration à partir de laquelle aucune simulation n'a encore été lancée. Dans chaque nœud, on stocke deux nombres : le nombre de simulations gagnantes et le nombre total de simulations. La méthode Monte-Carlo se déroule en quatre étapes\cite{MCTS} :

\begin{itemize}

    \item Sélection :
    
    Depuis la racine, on sélectionne successivement des enfants jusqu'à atteindre la fin de l'arbre. Dans cette phase, le choix des enfants est guidé par un compromis entre exploitation (choisir un noeud prouvé comme étant prometteur) et exploration (choisir un noeud moins prometteur en apparence afin de découvrir de nouvelles configurations qui peuvent se révéler être intéressant).
    
    \item Expansion :
    
    Si cette configuration n'est pas finale, créer de nouveaux enfants à mesure du possible en utilisant les règles du jeu et choisir l'un des enfants.
    
    \item Simulation :
    
    Simulez une partie au hasard depuis cet enfant, jusqu'à atteindre une configuration finale.
    
    \item Rétropropagation :
    
    Utiliser le résultat de la partie au hasard et mettre à jour les informations de manière rétroactive sur la branche en partant du noeud enfant jusqu'à la racine.
    
\end{itemize}

Dans AlphaGo, Monte-Carlo s'aidait d'un réseau de neurones entrainé par des parties d'experts ainsi que du renforcement en se combattant lui-même. Seulement, ce dispositif était limité de par le fait que les données d'experts sont souvent rares et surtout, elles sont onéreuses. Dans la version Zero, il se débarrasse de la méthode de recherche arborescente Monte-Carlo pour un autre algorithme\cite{ALPHAGOZERO} de recherche arborescente qui va s'abstraire de toutes connaissances, comme des parties étudiées, sauf les règles du jeu.

\subsection{Zero}

AlphaZero sera la version finale de AlphaGo. Débarrassé de la complexité de la recherche arborescente de Monte-Carlo et du réseau de neurones qui lui sert à étudier les parties d'experts, AlphaZero devient plus léger et plus généraliste dans sa façon d'apprendre.

La version AlphaGo Zero se base uniquement sur des parties jouées contre elle-même en utilisant un réseau de neurones qui concentre en un seul les deux réseaux de valeurs et d'objectifs qu'utilisait AlphaGo. De plus, elle utilise un nouvel algorithme d'apprentissage où elle apprend de ses propres mouvements, jusqu'à ce qu'elle puisse les anticiper et calculer de ces mouvements et leur impact sur le jeu. Cette version ne se sert plus de parties jouées par des experts ni de l'aide humaine.

La version d'AlphaZero reprend tous ces principes à quelques différences près :

\begin{itemize}
    \item AlphaZero a des règles codées en dur pour l'algorithme de recherche
    \item Le réseau de neurones est mis à jour de manière constante
    \item AlphaZero n'est pas programmé pour prendre avantage de la symétrie présente dans le Go
    \item AlphaZero prend en compte les matchs nuls
\end{itemize}
